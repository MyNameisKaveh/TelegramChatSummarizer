import os
import asyncio
import logging
from datetime import datetime, timedelta
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import re
import nltk
from nltk.tokenize import sent_tokenize

# ØªÙ†Ø¸ÛŒÙ… cache directory
cache_dir = '/tmp/transformers_cache'
os.environ['TRANSFORMERS_CACHE'] = cache_dir
os.environ['HF_HOME'] = cache_dir
os.makedirs(cache_dir, exist_ok=True)

# Ø¯Ø§Ù†Ù„ÙˆØ¯ nltk data
try:
    nltk.download('punkt', download_dir='./nltk_data', quiet=True)
    nltk.data.path.append('./nltk_data')
except:
    pass

# ØªÙ†Ø¸ÛŒÙ… logging
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

# Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆÙ‚ØªÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú¯Ø±ÙˆÙ‡
message_storage = {}
MAX_MESSAGES_PER_CHAT = 1000

# Ù…Ø¯Ù„ Ø³Ø¨Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ CPU Ø±Ø§ÛŒÚ¯Ø§Ù†
MODEL_NAME = "sshleifer/distilbart-cnn-6-6"  # Ø®ÛŒÙ„ÛŒ Ø³Ø¨Ú©â€ŒØªØ± Ø§Ø² bart-large
model = None
tokenizer = None

class MessageStore:
    """Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§"""
    
    def __init__(self):
        self.messages = {}
    
    def add_message(self, chat_id: int, user_id: int, username: str, text: str, timestamp: datetime):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯"""
        if chat_id not in self.messages:
            self.messages[chat_id] = []
        
        # Ø­ÙØ¸ Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…
        if len(self.messages[chat_id]) >= MAX_MESSAGES_PER_CHAT:
            self.messages[chat_id] = self.messages[chat_id][-MAX_MESSAGES_PER_CHAT//2:]
        
        self.messages[chat_id].append({
            'user_id': user_id,
            'username': username,
            'text': text,
            'timestamp': timestamp
        })
    
    def get_messages(self, chat_id: int, count: int = 50, hours_back: int = None):
        """Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¨Ø±Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§ Ø²Ù…Ø§Ù†"""
        if chat_id not in self.messages:
            return []
        
        messages = self.messages[chat_id]
        
        # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
        if hours_back:
            cutoff_time = datetime.now() - timedelta(hours=hours_back)
            messages = [msg for msg in messages if msg['timestamp'] >= cutoff_time]
        
        # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
        return messages[-count:] if count else messages

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø®Ø²Ù† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
message_store = MessageStore()

def load_lightweight_model():
    """Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ Ø³Ø¨Ú©"""
    try:
        logger.info(f"Loading lightweight model: {MODEL_NAME}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            cache_dir=cache_dir,
            local_files_only=False
        )
        
        model = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_NAME,
            cache_dir=cache_dir,
            local_files_only=False
        )
        
        logger.info("Model loaded successfully")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        # Ù…Ø¯Ù„ ÙÙˆÙ‚ Ø³Ø¨Ú© Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
        try:
            logger.info("Trying ultra-light model...")
            alt_model = "sshleifer/distilbart-cnn-2-2"
            
            tokenizer = AutoTokenizer.from_pretrained(alt_model, cache_dir=cache_dir)
            model = AutoModelForSeq2SeqLM.from_pretrained(alt_model, cache_dir=cache_dir)
            
            return model, tokenizer
        except Exception as e2:
            logger.error(f"Failed to load any model: {e2}")
            return None, None

def preprocess_persian_text(text):
    """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = re.sub(r'\s+', ' ', text)  # Ú†Ù†Ø¯ÛŒÙ† ÙØ§ØµÙ„Ù‡ -> ÛŒÚ© ÙØ§ØµÙ„Ù‡
    text = re.sub(r'\n+', '\n', text)  # Ú†Ù†Ø¯ÛŒÙ† Ø®Ø· Ø¬Ø¯ÛŒØ¯ -> ÛŒÚ© Ø®Ø·
    
    # Ø­Ø°Ù timestamp Ùˆ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
    text = re.sub(r'\d{2}:\d{2}', '', text)  # Ø²Ù…Ø§Ù†
    text = re.sub(r'@\w+', '', text)  # Ù…Ù†Ø´Ù†â€ŒÙ‡Ø§
    
    return text.strip()

def chunk_text_smart(text, max_length=400):
    """ØªÙ‚Ø³ÛŒÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…ØªÙ†"""
    try:
        sentences = sent_tokenize(text)
    except:
        sentences = re.split(r'[.!?]+', text)
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk + sentence) < max_length:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " " if len(sentence) < max_length else sentence[:max_length]
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def summarize_messages(messages_data):
    """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú¯Ø±ÙˆÙ‡"""
    global model, tokenizer
    
    if not model or not tokenizer:
        return "âŒ Ù…Ø¯Ù„ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
    
    if not messages_data:
        return "âŒ Ù¾ÛŒØ§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯"
    
    try:
        # ØªØ±Ú©ÛŒØ¨ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
        combined_text = ""
        for msg in messages_data:
            user_prefix = f"{msg['username'] or 'Ú©Ø§Ø±Ø¨Ø±'}: " if msg['username'] else ""
            combined_text += f"{user_prefix}{msg['text']}\n"
        
        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        combined_text = preprocess_persian_text(combined_text)
        
        if len(combined_text) < 100:
            return "âŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª"
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©
        chunks = chunk_text_smart(combined_text, max_length=350)
        summaries = []
        
        for i, chunk in enumerate(chunks[:3]):  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ø¨Ø®Ø´ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² timeout
            try:
                inputs = tokenizer.encode(
                    chunk,
                    return_tensors="pt",
                    max_length=400,
                    truncation=True
                )
                
                # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø³Ø¨Ú©â€ŒØªØ± Ø¨Ø±Ø§ÛŒ CPU
                summary_ids = model.generate(
                    inputs,
                    max_length=80,  # Ú©ÙˆØªØ§Ù‡â€ŒØªØ±
                    min_length=20,
                    length_penalty=1.5,  # Ú©Ù…ØªØ±
                    num_beams=2,  # Ú©Ù…ØªØ±
                    early_stopping=True,
                    no_repeat_ngram_size=2
                )
                
                summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
                summaries.append(summary)
                
            except Exception as e:
                logger.error(f"Error summarizing chunk {i}: {e}")
                continue
        
        if not summaries:
            return "âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ"
        
        # ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ
        final_summary = "\n\n".join(summaries)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù…Ø§Ø±ÛŒ
        stats = f"\n\nğŸ“Š Ø¢Ù…Ø§Ø±: {len(messages_data)} Ù¾ÛŒØ§Ù…ØŒ {len(combined_text)} Ú©Ø§Ø±Ø§Ú©ØªØ±"
        
        return f"ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ú¯ÙØªÚ¯Ùˆ:\n\n{final_summary}{stats}"
        
    except Exception as e:
        logger.error(f"Summarization error: {e}")
        return f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ: {str(e)}"

def parse_summary_request(text):
    """ØªØ¬Ø²ÛŒÙ‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ"""
    text = text.lower()
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…
    count_patterns = [
        r'(\d+)\s*Ù¾ÛŒØ§Ù…',
        r'(\d+)\s*ØªØ§',
        r'Ø¢Ø®Ø±ÛŒÙ†\s*(\d+)',
    ]
    
    message_count = 50  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
    
    for pattern in count_patterns:
        match = re.search(pattern, text)
        if match:
            message_count = min(int(match.group(1)), 200)  # Ø­Ø¯Ø§Ú©Ø«Ø± 200
            break
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
    time_patterns = [
        r'(\d+)\s*Ø³Ø§Ø¹Øª',
        r'(\d+)\s*Ø±ÙˆØ²',
    ]
    
    hours_back = None
    
    for pattern in time_patterns:
        match = re.search(pattern, text)
        if match:
            if 'Ø±ÙˆØ²' in pattern:
                hours_back = int(match.group(1)) * 24
            else:
                hours_back = int(match.group(1))
            hours_back = min(hours_back, 72)  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ø±ÙˆØ²
            break
    
    return message_count, hours_back

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø´Ø±ÙˆØ¹ Ø±Ø¨Ø§Øª"""
    welcome_msg = """
ğŸ¤– Ø³Ù„Ø§Ù…! Ù…Ù† Ø±Ø¨Ø§Øª Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø² Ú¯Ø±ÙˆÙ‡ Ù‡Ø³ØªÙ….

ğŸ“‹ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†:
- Ù…Ù† Ø±Ø§ Ø¨Ø§ @{} ØªÚ¯ Ú©Ù†ÛŒØ¯
- Ø¨Ø¹Ø¯ Ø¹Ø¨Ø§Ø±Øª "Ø®Ù„Ø§ØµÙ‡" ÛŒØ§ "Ø®Ù„Ø§ØµÙ‡ Ú©Ù†" Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
    
ğŸ”¹ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§:
â€¢ @{} Ø®Ù„Ø§ØµÙ‡ Ú©Ù†
â€¢ @{} Ø®Ù„Ø§ØµÙ‡ 100 Ù¾ÛŒØ§Ù… Ø¢Ø®Ø±
â€¢ @{} Ø®Ù„Ø§ØµÙ‡ 2 Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±
â€¢ @{} Ø®Ù„Ø§ØµÙ‡ Ú©Ù† Ø¢Ø®Ø±ÛŒÙ† 50 Ù¾ÛŒØ§Ù…

âš™ï¸ Ø¯Ø³ØªÙˆØ±Ø§Øª:
/help - Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„
/stats - Ø¢Ù…Ø§Ø± Ú¯Ø±ÙˆÙ‡

ğŸ”¸ ØªÙˆØ¬Ù‡: Ù…Ù† ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ ØªÚ¯ Ø´ÙˆÙ… Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…!
    """.format(
        context.bot.username,
        context.bot.username, 
        context.bot.username,
        context.bot.username
    )
    
    await update.message.reply_text(welcome_msg)

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„"""
    help_text = f"""
ğŸ¤– Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø±Ø¨Ø§Øª Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²

ğŸ“ Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡:
1. Ù…Ù† Ø±Ø§ Ø¨Ø§ @{context.bot.username} ØªÚ¯ Ú©Ù†ÛŒØ¯
2. Ú©Ù„Ù…Ù‡ "Ø®Ù„Ø§ØµÙ‡" ÛŒØ§ "Ø®Ù„Ø§ØµÙ‡ Ú©Ù†" Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
3. Ø§Ø®ØªÛŒØ§Ø±ÛŒ: ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù… ÛŒØ§ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯

ğŸ”¹ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù:
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ Ú©Ù†
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ 50 Ù¾ÛŒØ§Ù…
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ 3 Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ Ú©Ù† Ø¢Ø®Ø±ÛŒÙ† 100 ØªØ§

âš¡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
â€¢ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ§ 200 Ù¾ÛŒØ§Ù…
â€¢ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ØªØ§ 3 Ø±ÙˆØ²
â€¢ Ø­ÙØ¸ Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±Ø§Ù† Ø¯Ø± Ø®Ù„Ø§ØµÙ‡
â€¢ Ø¢Ù…Ø§Ø±Ú¯ÛŒØ±ÛŒ Ø§Ø² Ú¯ÙØªÚ¯Ùˆ

ğŸ”¸ Ù†Ú©ØªÙ‡: Ù…Ù† ÙÙ‚Ø· Ø¯Ø± Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù… Ùˆ ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ ØªÚ¯ Ø´ÙˆÙ…!
    """
    await update.message.reply_text(help_text)

async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ú¯Ø±ÙˆÙ‡"""
    chat_id = update.effective_chat.id
    
    if chat_id not in message_store.messages:
        await update.message.reply_text("ğŸ“Š Ù‡Ù†ÙˆØ² Ù¾ÛŒØ§Ù…ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ø´Ø¯Ù‡")
        return
    
    messages = message_store.messages[chat_id]
    total_messages = len(messages)
    
    # Ø´Ù…Ø§Ø±Ø´ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¯Ø± 24 Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±
    day_ago = datetime.now() - timedelta(hours=24)
    recent_messages = len([m for m in messages if m['timestamp'] >= day_ago])
    
    # Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙØ¹Ø§Ù„
    users = {}
    for msg in messages:
        username = msg['username'] or 'Ú©Ø§Ø±Ø¨Ø± Ù†Ø§Ø´Ù†Ø§Ø³'
        users[username] = users.get(username, 0) + 1
    
    top_users = sorted(users.items(), key=lambda x: x[1], reverse=True)[:5]
    
    stats_text = f"""
ğŸ“Š Ø¢Ù…Ø§Ø± Ú¯Ø±ÙˆÙ‡:

ğŸ“ˆ Ú©Ù„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡: {total_messages}
ğŸ• Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ 24 Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±: {recent_messages}

ğŸ‘¥ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙØ¹Ø§Ù„:
"""
    
    for username, count in top_users:
        stats_text += f"â€¢ {username}: {count} Ù¾ÛŒØ§Ù…\n"
    
    await update.message.reply_text(stats_text)

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡"""
    
    # Ø§Ú¯Ø± Ù¾ÛŒØ§Ù… Ø®ØµÙˆØµÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ±
    if update.effective_chat.type == 'private':
        return
    
    chat_id = update.effective_chat.id
    user_id = update.effective_user.id
    username = update.effective_user.username
    message_text = update.message.text
    timestamp = datetime.now()
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù… (Ù‡Ù…ÛŒØ´Ù‡)
    message_store.add_message(chat_id, user_id, username, message_text, timestamp)
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø±Ø¨Ø§Øª ØªÚ¯ Ø´Ø¯Ù‡ ÛŒØ§ Ù†Ù‡
    if not context.bot.username:
        return
        
    bot_mention = f"@{context.bot.username.lower()}"
    message_lower = message_text.lower()
    
    # Ø§Ú¯Ø± Ø±Ø¨Ø§Øª ØªÚ¯ Ù†Ø´Ø¯Ù‡ØŒ Ú©Ø§Ø±ÛŒ Ù†Ú©Ù†
    if bot_mention not in message_lower:
        return
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®Ù„Ø§ØµÙ‡
    summary_keywords = ['Ø®Ù„Ø§ØµÙ‡', 'Ø®Ù„Ø§ØµÙ‡ Ú©Ù†', 'summarize', 'Ø®Ù„Ø§ØµÙ‡ Ø¨Ø¯Ù‡']
    
    if not any(keyword in message_lower for keyword in summary_keywords):
        return
    
    # Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… "Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´"
    processing_msg = await update.message.reply_text("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§...")
    
    try:
        # ØªØ¬Ø²ÛŒÙ‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        message_count, hours_back = parse_summary_request(message_text)
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
        messages_data = message_store.get_messages(chat_id, message_count, hours_back)
        
        if not messages_data:
            await processing_msg.edit_text("âŒ Ù¾ÛŒØ§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return
        
        # Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
        summary = summarize_messages(messages_data)
        
        # Ø§Ø±Ø³Ø§Ù„ Ù†ØªÛŒØ¬Ù‡
        await processing_msg.edit_text(summary)
        
    except Exception as e:
        logger.error(f"Error in handle_message: {e}")
        await processing_msg.edit_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {str(e)}")

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§"""
    logger.error(f"Update {update} caused error {context.error}")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    global model, tokenizer
    
    # Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÚ©Ù†
    BOT_TOKEN = os.environ.get('BOT_TOKEN')
    if not BOT_TOKEN:
        logger.error("BOT_TOKEN not found!")
        return
    
    # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„
    logger.info("Loading model...")
    model, tokenizer = load_lightweight_model()
    
    if not model:
        logger.error("Failed to load model!")
        return
    
    # Ø³Ø§Ø®Øª Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† handlers
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(CommandHandler("stats", stats_command))
    
    # Handler Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ (Ø°Ø®ÛŒØ±Ù‡ + Ù¾Ø±Ø¯Ø§Ø²Ø´)
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    # Error handler
    app.add_error_handler(error_handler)
    
    # Ø´Ø±ÙˆØ¹
    logger.info("Bot started! Waiting for messages...")
    app.run_polling(drop_pending_updates=True)

if __name__ == '__main__':
    main()
