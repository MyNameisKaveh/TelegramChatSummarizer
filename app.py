import os
import asyncio
import logging
from datetime import datetime, timedelta
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
import re
import nltk
from nltk.tokenize import sent_tokenize
import torch

# ØªÙ†Ø¸ÛŒÙ… cache directory
cache_dir = '/tmp/transformers_cache'
os.environ['TRANSFORMERS_CACHE'] = cache_dir
os.environ['HF_HOME'] = cache_dir
os.makedirs(cache_dir, exist_ok=True)

# Ø¯Ø§Ù†Ù„ÙˆØ¯ nltk data
try:
    nltk.download('punkt', download_dir='./nltk_data', quiet=True)
    nltk.data.path.append('./nltk_data')
except:
    pass

# ØªÙ†Ø¸ÛŒÙ… logging
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

# Ø­Ø§ÙØ¸Ù‡ Ù…ÙˆÙ‚ØªÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ú¯Ø±ÙˆÙ‡
message_storage = {}
MAX_MESSAGES_PER_CHAT = 1000

# Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ
MODEL_NAME = "nafisehNik/mt5-persian-summary"
model = None
tokenizer = None

class MessageStore:
    """Ú©Ù„Ø§Ø³ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ùˆ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§"""
    
    def __init__(self):
        self.messages = {}
    
    def add_message(self, chat_id: int, user_id: int, username: str, text: str, timestamp: datetime):
        """Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… Ø¬Ø¯ÛŒØ¯"""
        if chat_id not in self.messages:
            self.messages[chat_id] = []
        
        # Ø­ÙØ¸ Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…
        if len(self.messages[chat_id]) >= MAX_MESSAGES_PER_CHAT:
            self.messages[chat_id] = self.messages[chat_id][-MAX_MESSAGES_PER_CHAT//2:]
        
        self.messages[chat_id].append({
            'user_id': user_id,
            'username': username,
            'text': text,
            'timestamp': timestamp
        })
    
    def get_messages(self, chat_id: int, count: int = 50, hours_back: int = None):
        """Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¨Ø±Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ ÛŒØ§ Ø²Ù…Ø§Ù†"""
        if chat_id not in self.messages:
            return []
        
        messages = self.messages[chat_id]
        
        # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø²Ù…Ø§Ù†
        if hours_back:
            cutoff_time = datetime.now() - timedelta(hours=hours_back)
            messages = [msg for msg in messages if msg['timestamp'] >= cutoff_time]
        
        # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
        return messages[-count:] if count else messages

# Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù…Ø®Ø²Ù† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
message_store = MessageStore()

def load_persian_model():
    """Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ"""
    try:
        logger.info(f"Loading Persian model: {MODEL_NAME}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            cache_dir=cache_dir,
            local_files_only=False
        )
        
        model = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_NAME,
            cache_dir=cache_dir,
            local_files_only=False,
            torch_dtype=torch.float32,
            low_cpu_mem_usage=True,
        )
        
        model.eval()
        logger.info("Persian model loaded successfully")
        return model, tokenizer
        
    except Exception as e:
        logger.error(f"Error loading Persian model: {e}")
        return None, None

def preprocess_persian_text(text):
    """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ"""
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ùˆ ØªÙ…ÛŒØ² Ú©Ø±Ø¯Ù†
    text = re.sub(r'\s+', ' ', text)  # Ú†Ù†Ø¯ÛŒÙ† ÙØ§ØµÙ„Ù‡ -> ÛŒÚ© ÙØ§ØµÙ„Ù‡
    text = re.sub(r'\n+', '\n', text)  # Ú†Ù†Ø¯ÛŒÙ† Ø®Ø· Ø¬Ø¯ÛŒØ¯ -> ÛŒÚ© Ø®Ø·
    
    # Ø­Ø°Ù timestamp Ùˆ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ú©Ø§Ø±Ø¨Ø±ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…
    text = re.sub(r'\d{2}:\d{2}', '', text)  # Ø²Ù…Ø§Ù†
    text = re.sub(r'@\w+', '', text)  # Ù…Ù†Ø´Ù†â€ŒÙ‡Ø§
    
    # Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§
    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
    
    # Ø­Ø°Ù Ø§ÛŒÙ…ÙˆØ¬ÛŒâ€ŒÙ‡Ø§
    text = re.sub(r'[^\w\s\u0600-\u06FF\u0750-\u077F\u08A0-\u08FF\uFB50-\uFDFF\uFE70-\uFEFF]', ' ', text)
    
    return text.strip()

def chunk_text_smart(text, max_length=300):
    """ØªÙ‚Ø³ÛŒÙ… Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…ØªÙ† Ø¨Ø§ Ø¯Ø± Ù†Ø¸Ø± Ú¯ÛŒØ±ÛŒ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ"""
    try:
        sentences = sent_tokenize(text)
    except:
        # Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ù„Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§Ø±Ø³ÛŒ
        sentences = re.split(r'[.!?ØŸÛ”]+', text)
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        sentence = sentence.strip()
        if not sentence:
            continue
            
        if len(current_chunk + sentence) < max_length:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " " if len(sentence) < max_length else sentence[:max_length]
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def summarize_messages(messages_data):
    """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ú¯Ø±ÙˆÙ‡ Ø¨Ø§ Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ"""
    global model, tokenizer
    
    if not model or not tokenizer:
        return "âŒ Ù…Ø¯Ù„ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª"
    
    if not messages_data:
        return "âŒ Ù¾ÛŒØ§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯"
    
    try:
        # ØªØ±Ú©ÛŒØ¨ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
        combined_text = ""
        for msg in messages_data:
            user_prefix = f"{msg['username'] or 'Ú©Ø§Ø±Ø¨Ø±'}: " if msg['username'] else ""
            combined_text += f"{user_prefix}{msg['text']}\n"
        
        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        combined_text = preprocess_persian_text(combined_text)
        
        if len(combined_text) < 100:
            return "âŒ Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª"
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ø¨Ø®Ø´â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©
        chunks = chunk_text_smart(combined_text, max_length=400)
        summaries = []
        
        for i, chunk in enumerate(chunks[:2]):  # Ø­Ø¯Ø§Ú©Ø«Ø± 2 Ø¨Ø®Ø´
            try:
                inputs = tokenizer.encode(
                    f"Ø®Ù„Ø§ØµÙ‡: {chunk}",
                    return_tensors="pt",
                    max_length=512,
                    truncation=True
                )
                
                summary_ids = model.generate(
                    inputs,
                    max_length=100,
                    min_length=30,
                    length_penalty=1.2,
                    num_beams=3,
                    early_stopping=True,
                    no_repeat_ngram_size=3
                )
                
                summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
                
                # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† prefix
                if summary.startswith("Ø®Ù„Ø§ØµÙ‡:"):
                    summary = summary[5:].strip()
                
                summaries.append(summary)
                
            except Exception as e:
                logger.error(f"Error summarizing chunk {i}: {e}")
                continue
        
        if not summaries:
            return "âŒ Ø®Ø·Ø§ Ø¯Ø± ÙØ±Ø¢ÛŒÙ†Ø¯ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ"
        
        # ØªØ±Ú©ÛŒØ¨ Ù†Ù‡Ø§ÛŒÛŒ
        final_summary = "\n\n".join(summaries)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¢Ù…Ø§Ø±ÛŒ
        stats = f"\n\nğŸ“Š Ø¢Ù…Ø§Ø±: {len(messages_data)} Ù¾ÛŒØ§Ù…ØŒ {len(combined_text)} Ú©Ø§Ø±Ø§Ú©ØªØ±"
        
        return f"ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ú¯ÙØªÚ¯Ùˆ:\n\n{final_summary}{stats}"
        
    except Exception as e:
        logger.error(f"Summarization error: {e}")
        return f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ: {str(e)}"

def parse_summary_request(text):
    """ØªØ¬Ø²ÛŒÙ‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ"""
    text = text.lower()
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù…
    count_patterns = [
        r'(\d+)\s*Ù¾ÛŒØ§Ù…',
        r'(\d+)\s*ØªØ§',
        r'Ø¢Ø®Ø±ÛŒÙ†\s*(\d+)',
    ]
    
    message_count = 50  # Ù¾ÛŒØ´â€ŒÙØ±Ø¶
    
    for pattern in count_patterns:
        match = re.search(pattern, text)
        if match:
            message_count = min(int(match.group(1)), 200)  # Ø­Ø¯Ø§Ú©Ø«Ø± 200
            break
    
    # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ
    time_patterns = [
        r'(\d+)\s*Ø³Ø§Ø¹Øª',
        r'(\d+)\s*Ø±ÙˆØ²',
    ]
    
    hours_back = None
    
    for pattern in time_patterns:
        match = re.search(pattern, text)
        if match:
            if 'Ø±ÙˆØ²' in pattern:
                hours_back = int(match.group(1)) * 24
            else:
                hours_back = int(match.group(1))
            hours_back = min(hours_back, 72)  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 Ø±ÙˆØ²
            break
    
    return message_count, hours_back

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø´Ø±ÙˆØ¹ Ø±Ø¨Ø§Øª"""
    welcome_msg = f"""
ğŸ¤– Ø³Ù„Ø§Ù…! Ù…Ù† Ø±Ø¨Ø§Øª Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø² Ú¯Ø±ÙˆÙ‡ Ù‡Ø³ØªÙ….

ğŸ“‹ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ù†:
- Ù…Ù† Ø±Ø§ Ø¨Ø§ @{context.bot.username} ØªÚ¯ Ú©Ù†ÛŒØ¯
- Ø¨Ø¹Ø¯ Ø¹Ø¨Ø§Ø±Øª "Ø®Ù„Ø§ØµÙ‡" ÛŒØ§ "Ø®Ù„Ø§ØµÙ‡ Ú©Ù†" Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯
    
ğŸ”¹ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§:
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ Ú©Ù†
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ 100 Ù¾ÛŒØ§Ù… Ø¢Ø®Ø±
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ 2 Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±

âš™ï¸ Ø¯Ø³ØªÙˆØ±Ø§Øª:
/help - Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„
/stats - Ø¢Ù…Ø§Ø± Ú¯Ø±ÙˆÙ‡
/model - Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„ ÙØ¹Ù„ÛŒ

ğŸ”¸ ØªÙˆØ¬Ù‡: Ù…Ù† ÙÙ‚Ø· ÙˆÙ‚ØªÛŒ ØªÚ¯ Ø´ÙˆÙ… Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…!
    """
    
    await update.message.reply_text(welcome_msg)

async def model_info(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„ ÙØ¹Ù„ÛŒ"""
    info_text = f"""
ğŸ¤– Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„ ÙØ¹Ù„ÛŒ:

ğŸ“¦ Ù†Ø§Ù… Ù…Ø¯Ù„: {MODEL_NAME}
ğŸŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø²Ø¨Ø§Ù†: âœ… ÙØ§Ø±Ø³ÛŒ
ğŸ’¾ ÙˆØ¶Ø¹ÛŒØª: ÙØ¹Ø§Ù„ Ùˆ Ø¢Ù…Ø§Ø¯Ù‡
    """
    await update.message.reply_text(info_text)

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù…Ù„"""
    help_text = f"""
ğŸ¤– Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø±Ø¨Ø§Øª Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²

ğŸ“ Ù†Ø­ÙˆÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡:
1. Ù…Ù† Ø±Ø§ Ø¨Ø§ @{context.bot.username} ØªÚ¯ Ú©Ù†ÛŒØ¯
2. Ú©Ù„Ù…Ù‡ "Ø®Ù„Ø§ØµÙ‡" ÛŒØ§ "Ø®Ù„Ø§ØµÙ‡ Ú©Ù†" Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
3. Ø§Ø®ØªÛŒØ§Ø±ÛŒ: ØªØ¹Ø¯Ø§Ø¯ Ù¾ÛŒØ§Ù… ÛŒØ§ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯

ğŸ”¹ Ù…Ø«Ø§Ù„â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù:
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ Ú©Ù†
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ 50 Ù¾ÛŒØ§Ù…
â€¢ @{context.bot.username} Ø®Ù„Ø§ØµÙ‡ 3 Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±

âš¡ ÙˆÛŒÚ˜Ú¯ÛŒâ€ŒÙ‡Ø§:
â€¢ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØ§ 200 Ù¾ÛŒØ§Ù…
â€¢ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ØªØ§ 3 Ø±ÙˆØ²
â€¢ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
â€¢ ØªØ·Ø¨ÛŒÙ‚ Ø®ÙˆØ¯Ú©Ø§Ø± Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ù…Ø¯Ù„ Ù…ÙˆØ¬ÙˆØ¯

ğŸ”§ Ø¯Ø³ØªÙˆØ±Ø§Øª:
/start - Ø´Ø±ÙˆØ¹
/help - Ø±Ø§Ù‡Ù†Ù…Ø§
/stats - Ø¢Ù…Ø§Ø± Ú¯Ø±ÙˆÙ‡
/model - Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø¯Ù„

ğŸ”¸ Ù†Ú©ØªÙ‡: Ù…Ù† ÙÙ‚Ø· Ø¯Ø± Ú¯Ø±ÙˆÙ‡â€ŒÙ‡Ø§ Ùˆ ÙˆÙ‚ØªÛŒ ØªÚ¯ Ø´ÙˆÙ… Ú©Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ù…!
    """
    await update.message.reply_text(help_text)

async def stats_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù†Ù…Ø§ÛŒØ´ Ø¢Ù…Ø§Ø± Ú¯Ø±ÙˆÙ‡"""
    chat_id = update.effective_chat.id
    
    if chat_id not in message_store.messages:
        await update.message.reply_text("ğŸ“Š Ù‡Ù†ÙˆØ² Ù¾ÛŒØ§Ù…ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ø´Ø¯Ù‡")
        return
    
    messages = message_store.messages[chat_id]
    total_messages = len(messages)
    
    # Ø´Ù…Ø§Ø±Ø´ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ø¯Ø± 24 Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±
    day_ago = datetime.now() - timedelta(hours=24)
    recent_messages = len([m for m in messages if m['timestamp'] >= day_ago])
    
    # Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙØ¹Ø§Ù„
    users = {}
    for msg in messages:
        username = msg['username'] or 'Ú©Ø§Ø±Ø¨Ø± Ù†Ø§Ø´Ù†Ø§Ø³'
        users[username] = users.get(username, 0) + 1
    
    top_users = sorted(users.items(), key=lambda x: x[1], reverse=True)[:5]
    
    stats_text = f"""
ğŸ“Š Ø¢Ù…Ø§Ø± Ú¯Ø±ÙˆÙ‡:

ğŸ“ˆ Ú©Ù„ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù‡: {total_messages}
ğŸ• Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ 24 Ø³Ø§Ø¹Øª Ø§Ø®ÛŒØ±: {recent_messages}

ğŸ‘¥ Ú©Ø§Ø±Ø¨Ø±Ø§Ù† ÙØ¹Ø§Ù„:
"""
    
    for username, count in top_users:
        stats_text += f"â€¢ {username}: {count} Ù¾ÛŒØ§Ù…\n"
    
    await update.message.reply_text(stats_text)

async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡"""
    
    # Ø§Ú¯Ø± Ù¾ÛŒØ§Ù… Ø®ØµÙˆØµÛŒ Ø¨Ø§Ø´Ø¯ØŒ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ±
    if update.effective_chat.type == 'private':
        return
    
    chat_id = update.effective_chat.id
    user_id = update.effective_user.id
    username = update.effective_user.username
    message_text = update.message.text
    timestamp = datetime.now()
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù¾ÛŒØ§Ù… (Ù‡Ù…ÛŒØ´Ù‡)
    message_store.add_message(chat_id, user_id, username, message_text, timestamp)
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø¢ÛŒØ§ Ø±Ø¨Ø§Øª ØªÚ¯ Ø´Ø¯Ù‡ ÛŒØ§ Ù†Ù‡
    if not context.bot.username:
        return
        
    bot_mention = f"@{context.bot.username.lower()}"
    message_lower = message_text.lower()
    
    # Ø§Ú¯Ø± Ø±Ø¨Ø§Øª ØªÚ¯ Ù†Ø´Ø¯Ù‡ØŒ Ú©Ø§Ø±ÛŒ Ù†Ú©Ù†
    if bot_mention not in message_lower:
        return
    
    # Ø¨Ø±Ø±Ø³ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®Ù„Ø§ØµÙ‡
    summary_keywords = ['Ø®Ù„Ø§ØµÙ‡', 'Ø®Ù„Ø§ØµÙ‡ Ú©Ù†', 'summarize', 'Ø®Ù„Ø§ØµÙ‡ Ø¨Ø¯Ù‡']
    
    if not any(keyword in message_lower for keyword in summary_keywords):
        return
    
    # Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… "Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´"
    processing_msg = await update.message.reply_text("â³ Ø¯Ø± Ø­Ø§Ù„ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§...")
    
    try:
        # ØªØ¬Ø²ÛŒÙ‡ Ø¯Ø±Ø®ÙˆØ§Ø³Øª
        message_count, hours_back = parse_summary_request(message_text)
        
        # Ø¯Ø±ÛŒØ§ÙØª Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
        messages_data = message_store.get_messages(chat_id, message_count, hours_back)
        
        if not messages_data:
            await processing_msg.edit_text("âŒ Ù¾ÛŒØ§Ù…ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
            return
        
        # Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
        summary = summarize_messages(messages_data)
        
        # Ø§Ø±Ø³Ø§Ù„ Ù†ØªÛŒØ¬Ù‡
        await processing_msg.edit_text(summary)
        
    except Exception as e:
        logger.error(f"Error in handle_message: {e}")
        await processing_msg.edit_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {str(e)}")

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§"""
    logger.error(f"Update {update} caused error {context.error}")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    global model, tokenizer
    
    # Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÚ©Ù†
    BOT_TOKEN = os.environ.get('BOT_TOKEN')
    if not BOT_TOKEN:
        logger.error("BOT_TOKEN not found!")
        return
    
    # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ
    logger.info("Loading Persian model...")
    model, tokenizer = load_persian_model()
    
    if not model:
        logger.error("Failed to load any model!")
        return
    
    # Ø³Ø§Ø®Øª Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† handlers
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(CommandHandler("stats", stats_command))
    app.add_handler(CommandHandler("model", model_info))
    
    # Handler Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ (Ø°Ø®ÛŒØ±Ù‡ + Ù¾Ø±Ø¯Ø§Ø²Ø´)
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))
    
    # Error handler
    app.add_error_handler(error_handler)
    
    # Ø´Ø±ÙˆØ¹
    logger.info(f"Bot started with Persian model: {MODEL_NAME}")
    app.run_polling(drop_pending_updates=True)

if __name__ == '__main__':
    main()
