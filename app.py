import os
import asyncio
import logging
from telegram import Update
from telegram.ext import ApplicationBuilder, CommandHandler, MessageHandler, filters, ContextTypes
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline
import nltk
from nltk.tokenize import sent_tokenize

# ØªÙ†Ø¸ÛŒÙ… cache directory Ø¨Ø±Ø§ÛŒ transformers - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² /tmp
cache_dir = '/tmp/transformers_cache'
os.environ['TRANSFORMERS_CACHE'] = cache_dir
os.environ['HF_HOME'] = cache_dir

# Ø§ÛŒØ¬Ø§Ø¯ Ø¯Ø§ÛŒØ±Ú©ØªÙˆØ±ÛŒ cache Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ù‡
os.makedirs(cache_dir, exist_ok=True)

# Ø¯Ø§Ù†Ù„ÙˆØ¯ nltk data
try:
    nltk.download('punkt', download_dir='./nltk_data')
    nltk.data.path.append('./nltk_data')
except:
    pass

# ØªÙ†Ø¸ÛŒÙ… logging
logging.basicConfig(format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', level=logging.INFO)
logger = logging.getLogger(__name__)

# Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„ - Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú©ØªØ±
MODEL_NAME = "facebook/bart-large-cnn"  # Ù…Ø¯Ù„ Ú©ÙˆÚ†Ú©ØªØ± Ùˆ Ø³Ø±ÛŒØ¹ØªØ±
model = None
tokenizer = None

def get_summarizer_model():
    """Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„ Ùˆ tokenizer"""
    try:
        cache_dir = '/tmp/transformers_cache'
        # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² cache directory Ø³ÙØ§Ø±Ø´ÛŒ
        model = AutoModelForSeq2SeqLM.from_pretrained(
            MODEL_NAME,
            cache_dir=cache_dir,
            local_files_only=False,
            trust_remote_code=True
        )
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME,
            cache_dir=cache_dir,
            local_files_only=False,
            trust_remote_code=True
        )
        return model, tokenizer
    except Exception as e:
        logger.error(f"Error loading model: {e}")
        # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ø§ØµÙ„ÛŒ Ú©Ø§Ø± Ù†Ú©Ø±Ø¯ØŒ Ø§Ø² Ù…Ø¯Ù„ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        try:
            logger.info("Trying alternative model...")
            alt_model_name = "sshleifer/distilbart-cnn-12-6"
            model = AutoModelForSeq2SeqLM.from_pretrained(
                alt_model_name,
                cache_dir=cache_dir,
                local_files_only=False
            )
            tokenizer = AutoTokenizer.from_pretrained(
                alt_model_name,
                cache_dir=cache_dir,
                local_files_only=False
            )
            return model, tokenizer
        except Exception as e2:
            logger.error(f"Error loading alternative model: {e2}")
            return None, None

def preprocess_text(text):
    """Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†"""
    # Ø­Ø°Ù Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = text.replace('\n', ' ').replace('\r', ' ')
    # Ø­Ø°Ù ÙØ§ØµÙ„Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
    text = ' '.join(text.split())
    return text

def chunk_text(text, max_length=512):
    """ØªÙ‚Ø³ÛŒÙ… Ù…ØªÙ† Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª Ú©ÙˆÚ†Ú©ØªØ±"""
    try:
        sentences = sent_tokenize(text)
    except:
        # Ø§Ú¯Ø± nltk Ú©Ø§Ø± Ù†Ú©Ø±Ø¯ØŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ø§Ø¯Ù‡ ØªÙ‚Ø³ÛŒÙ… Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        sentences = text.split('. ')
    
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if len(current_chunk + sentence) < max_length:
            current_chunk += sentence + " "
        else:
            if current_chunk:
                chunks.append(current_chunk.strip())
            current_chunk = sentence + " "
    
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks

def summarize_text(text):
    """Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªÙ†"""
    global model, tokenizer
    
    if model is None or tokenizer is None:
        return "Ù…Ø¯Ù„ Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª."
    
    try:
        # Ù¾ÛŒØ´â€ŒÙ¾Ø±Ø¯Ø§Ø²Ø´
        text = preprocess_text(text)
        
        if len(text) < 100:
            return "Ù…ØªÙ† Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø³ÛŒØ§Ø± Ú©ÙˆØªØ§Ù‡ Ø§Ø³Øª."
        
        # ØªÙ‚Ø³ÛŒÙ… Ø¨Ù‡ Ù‚Ø·Ø¹Ø§Øª
        chunks = chunk_text(text, max_length=400)
        summaries = []
        
        for chunk in chunks:
            inputs = tokenizer.encode(
                "summarize: " + chunk,
                return_tensors="pt",
                max_length=512,
                truncation=True
            )
            
            summary_ids = model.generate(
                inputs,
                max_length=150,
                min_length=40,
                length_penalty=2.0,
                num_beams=4,
                early_stopping=True
            )
            
            summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)
            summaries.append(summary)
        
        # ØªØ±Ú©ÛŒØ¨ Ø®Ù„Ø§ØµÙ‡â€ŒÙ‡Ø§
        final_summary = " ".join(summaries)
        return final_summary
        
    except Exception as e:
        logger.error(f"Error in summarization: {e}")
        return f"Ø®Ø·Ø§ Ø¯Ø± Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ: {str(e)}"

async def start(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø´Ø±ÙˆØ¹ Ø¨Ø§Øª"""
    await update.message.reply_text(
        "Ø³Ù„Ø§Ù…! Ù…Ù† Ø±Ø¨Ø§Øª Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø² Ù‡Ø³ØªÙ….\n"
        "Ù…ØªÙ† Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ù† Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯ ØªØ§ Ø¢Ù† Ø±Ø§ Ø®Ù„Ø§ØµÙ‡ Ú©Ù†Ù….\n"
        "Ø¨Ø±Ø§ÛŒ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ /help Ø±Ø§ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯."
    )

async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡"""
    help_text = """
ğŸ¤– Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø±Ø¨Ø§Øª Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²:

ğŸ“ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ:
- Ù…ØªÙ† Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯
- Ù…ØªÙ† Ø¨Ø§ÛŒØ¯ Ø­Ø¯Ø§Ù‚Ù„ Û±Û°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ø§Ø´Ø¯

âš¡ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:
- Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø²Ù…Ø§Ù† Ø¨ÛŒØ´ØªØ±ÛŒ Ù†ÛŒØ§Ø² Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯
- Ú©ÛŒÙÛŒØª Ø®Ù„Ø§ØµÙ‡ Ø¨Ø³ØªÚ¯ÛŒ Ø¨Ù‡ Ù…Ø­ØªÙˆØ§ÛŒ Ù…ØªÙ† Ø¯Ø§Ø±Ø¯

ğŸ“§ Ø¯Ø³ØªÙˆØ±Ø§Øª:
/start - Ø´Ø±ÙˆØ¹ Ù…Ø¬Ø¯Ø¯
/help - Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡
    """
    await update.message.reply_text(help_text)

async def handle_text(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ø§Ø±Ø³Ø§Ù„ÛŒ"""
    user_text = update.message.text
    
    if not user_text or len(user_text.strip()) < 50:
        await update.message.reply_text("Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ±ÛŒ Ø§Ø±Ø³Ø§Ù„ Ú©Ù†ÛŒØ¯ (Ø­Ø¯Ø§Ù‚Ù„ ÛµÛ° Ú©Ø§Ø±Ø§Ú©ØªØ±)")
        return
    
    # Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… "Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´"
    processing_message = await update.message.reply_text("â³ Ø¯Ø± Ø­Ø§Ù„ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ...")
    
    try:
        # Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ
        summary = summarize_text(user_text)
        
        # Ø§Ø±Ø³Ø§Ù„ Ù†ØªÛŒØ¬Ù‡
        result_text = f"ğŸ“ Ø®Ù„Ø§ØµÙ‡ Ù…ØªÙ†:\n\n{summary}"
        
        await processing_message.edit_text(result_text)
        
    except Exception as e:
        logger.error(f"Error handling text: {e}")
        await processing_message.edit_text(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´: {str(e)}")

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE):
    """Ù…Ø¯ÛŒØ±ÛŒØª Ø®Ø·Ø§Ù‡Ø§"""
    logger.error(f"Update {update} caused error {context.error}")

def main():
    """ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ"""
    global model, tokenizer
    
    # Ø¯Ø±ÛŒØ§ÙØª ØªÙˆÚ©Ù† Ø§Ø² Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ
    BOT_TOKEN = os.environ.get('BOT_TOKEN')
    if not BOT_TOKEN:
        logger.error("BOT_TOKEN not found in environment variables")
        return
    
    # Ø¨Ø§Ø±Ú¯ÛŒØ±ÛŒ Ù…Ø¯Ù„
    logger.info("Loading model...")
    model, tokenizer = get_summarizer_model()
    
    if model is None:
        logger.error("Failed to load model")
        return
    
    logger.info("Model loaded successfully")
    
    # Ø³Ø§Ø®Øª Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†
    app = ApplicationBuilder().token(BOT_TOKEN).build()
    
    # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† handlers
    app.add_handler(CommandHandler("start", start))
    app.add_handler(CommandHandler("help", help_command))
    app.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_text))
    app.add_error_handler(error_handler)
    
    # Ø´Ø±ÙˆØ¹ polling
    logger.info("Starting bot...")
    app.run_polling(drop_pending_updates=True)

if __name__ == '__main__':
    main()
